{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "sublime-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas\n",
    "import numpy\n",
    "import hvplot.pandas\n",
    "\n",
    "#machine learning dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "#connection to db\n",
    "from connect_sql_db import build_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-colors",
   "metadata": {},
   "source": [
    "# Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look into polynomial linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-playing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-croatia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "running-butler",
   "metadata": {},
   "source": [
    "# Neural Network Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "changed-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = build_engine(database_name=\"database1\",host=\"35.225.193.21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dramatic-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pandas.read_sql(\"select * from cleaned_table\",con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bearing-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df[\"stars\"] = cleaned_df.stars.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "interim-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_df.drop(cleaned_df.dtypes[cleaned_df.dtypes == \"object\"].index.tolist(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "mechanical-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cleaned_df.stars\n",
    "X = X.drop(\"stars\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "chubby-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2581\n",
       "3    2395\n",
       "2     431\n",
       "1      50\n",
       "5      43\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "narrow-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    random_state = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "committed-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train.values.reshape(-1, 1))\n",
    "encoded_y_train = enc.transform(y_train.values.reshape(-1, 1)).toarray()\n",
    "encoded_y_test = enc.transform(y_train.values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "spread-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "brave-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100, activation='relu', input_dim=X_train_scaled.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "touched-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "appreciated-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "southern-indonesian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 100)               5000      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "complete-senator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "129/129 - 0s - loss: 1.1301 - accuracy: 0.5045\n",
      "Epoch 2/100\n",
      "129/129 - 0s - loss: 0.9085 - accuracy: 0.5789\n",
      "Epoch 3/100\n",
      "129/129 - 0s - loss: 0.8724 - accuracy: 0.5995\n",
      "Epoch 4/100\n",
      "129/129 - 0s - loss: 0.8503 - accuracy: 0.6160\n",
      "Epoch 5/100\n",
      "129/129 - 0s - loss: 0.8332 - accuracy: 0.6196\n",
      "Epoch 6/100\n",
      "129/129 - 0s - loss: 0.8209 - accuracy: 0.6252\n",
      "Epoch 7/100\n",
      "129/129 - 0s - loss: 0.8078 - accuracy: 0.6330\n",
      "Epoch 8/100\n",
      "129/129 - 0s - loss: 0.7992 - accuracy: 0.6325\n",
      "Epoch 9/100\n",
      "129/129 - 0s - loss: 0.7883 - accuracy: 0.6429\n",
      "Epoch 10/100\n",
      "129/129 - 0s - loss: 0.7801 - accuracy: 0.6541\n",
      "Epoch 11/100\n",
      "129/129 - 0s - loss: 0.7703 - accuracy: 0.6524\n",
      "Epoch 12/100\n",
      "129/129 - 0s - loss: 0.7595 - accuracy: 0.6616\n",
      "Epoch 13/100\n",
      "129/129 - 0s - loss: 0.7575 - accuracy: 0.6596\n",
      "Epoch 14/100\n",
      "129/129 - 0s - loss: 0.7461 - accuracy: 0.6664\n",
      "Epoch 15/100\n",
      "129/129 - 0s - loss: 0.7408 - accuracy: 0.6669\n",
      "Epoch 16/100\n",
      "129/129 - 0s - loss: 0.7330 - accuracy: 0.6732\n",
      "Epoch 17/100\n",
      "129/129 - 0s - loss: 0.7280 - accuracy: 0.6747\n",
      "Epoch 18/100\n",
      "129/129 - 0s - loss: 0.7234 - accuracy: 0.6815\n",
      "Epoch 19/100\n",
      "129/129 - 0s - loss: 0.7172 - accuracy: 0.6785\n",
      "Epoch 20/100\n",
      "129/129 - 0s - loss: 0.7105 - accuracy: 0.6853\n",
      "Epoch 21/100\n",
      "129/129 - 0s - loss: 0.7074 - accuracy: 0.6834\n",
      "Epoch 22/100\n",
      "129/129 - 0s - loss: 0.7013 - accuracy: 0.6953\n",
      "Epoch 23/100\n",
      "129/129 - 0s - loss: 0.6969 - accuracy: 0.6912\n",
      "Epoch 24/100\n",
      "129/129 - 0s - loss: 0.6934 - accuracy: 0.6924\n",
      "Epoch 25/100\n",
      "129/129 - 0s - loss: 0.6891 - accuracy: 0.6941\n",
      "Epoch 26/100\n",
      "129/129 - 0s - loss: 0.6796 - accuracy: 0.7040\n",
      "Epoch 27/100\n",
      "129/129 - 0s - loss: 0.6782 - accuracy: 0.7050\n",
      "Epoch 28/100\n",
      "129/129 - 0s - loss: 0.6747 - accuracy: 0.7072\n",
      "Epoch 29/100\n",
      "129/129 - 0s - loss: 0.6687 - accuracy: 0.7084\n",
      "Epoch 30/100\n",
      "129/129 - 0s - loss: 0.6642 - accuracy: 0.7118\n",
      "Epoch 31/100\n",
      "129/129 - 0s - loss: 0.6618 - accuracy: 0.7098\n",
      "Epoch 32/100\n",
      "129/129 - 0s - loss: 0.6562 - accuracy: 0.7183\n",
      "Epoch 33/100\n",
      "129/129 - 0s - loss: 0.6531 - accuracy: 0.7084\n",
      "Epoch 34/100\n",
      "129/129 - 0s - loss: 0.6492 - accuracy: 0.7173\n",
      "Epoch 35/100\n",
      "129/129 - 0s - loss: 0.6444 - accuracy: 0.7183\n",
      "Epoch 36/100\n",
      "129/129 - 0s - loss: 0.6405 - accuracy: 0.7212\n",
      "Epoch 37/100\n",
      "129/129 - 0s - loss: 0.6357 - accuracy: 0.7193\n",
      "Epoch 38/100\n",
      "129/129 - 0s - loss: 0.6372 - accuracy: 0.7154\n",
      "Epoch 39/100\n",
      "129/129 - 0s - loss: 0.6295 - accuracy: 0.7251\n",
      "Epoch 40/100\n",
      "129/129 - 0s - loss: 0.6276 - accuracy: 0.7328\n",
      "Epoch 41/100\n",
      "129/129 - 0s - loss: 0.6264 - accuracy: 0.7341\n",
      "Epoch 42/100\n",
      "129/129 - 0s - loss: 0.6203 - accuracy: 0.7316\n",
      "Epoch 43/100\n",
      "129/129 - 0s - loss: 0.6183 - accuracy: 0.7345\n",
      "Epoch 44/100\n",
      "129/129 - 0s - loss: 0.6164 - accuracy: 0.7343\n",
      "Epoch 45/100\n",
      "129/129 - 0s - loss: 0.6121 - accuracy: 0.7302\n",
      "Epoch 46/100\n",
      "129/129 - 0s - loss: 0.6070 - accuracy: 0.7370\n",
      "Epoch 47/100\n",
      "129/129 - 0s - loss: 0.6038 - accuracy: 0.7370\n",
      "Epoch 48/100\n",
      "129/129 - 0s - loss: 0.6020 - accuracy: 0.7367\n",
      "Epoch 49/100\n",
      "129/129 - 0s - loss: 0.6010 - accuracy: 0.7389\n",
      "Epoch 50/100\n",
      "129/129 - 0s - loss: 0.5981 - accuracy: 0.7384\n",
      "Epoch 51/100\n",
      "129/129 - 0s - loss: 0.5968 - accuracy: 0.7418\n",
      "Epoch 52/100\n",
      "129/129 - 0s - loss: 0.5908 - accuracy: 0.7484\n",
      "Epoch 53/100\n",
      "129/129 - 0s - loss: 0.5876 - accuracy: 0.7455\n",
      "Epoch 54/100\n",
      "129/129 - 0s - loss: 0.5848 - accuracy: 0.7496\n",
      "Epoch 55/100\n",
      "129/129 - 0s - loss: 0.5831 - accuracy: 0.7484\n",
      "Epoch 56/100\n",
      "129/129 - 0s - loss: 0.5812 - accuracy: 0.7481\n",
      "Epoch 57/100\n",
      "129/129 - 0s - loss: 0.5799 - accuracy: 0.7496\n",
      "Epoch 58/100\n",
      "129/129 - 0s - loss: 0.5763 - accuracy: 0.7554\n",
      "Epoch 59/100\n",
      "129/129 - 0s - loss: 0.5759 - accuracy: 0.7503\n",
      "Epoch 60/100\n",
      "129/129 - 0s - loss: 0.5723 - accuracy: 0.7547\n",
      "Epoch 61/100\n",
      "129/129 - 0s - loss: 0.5686 - accuracy: 0.7547\n",
      "Epoch 62/100\n",
      "129/129 - 0s - loss: 0.5664 - accuracy: 0.7549\n",
      "Epoch 63/100\n",
      "129/129 - 0s - loss: 0.5660 - accuracy: 0.7593\n",
      "Epoch 64/100\n",
      "129/129 - 0s - loss: 0.5589 - accuracy: 0.7578\n",
      "Epoch 65/100\n",
      "129/129 - 0s - loss: 0.5577 - accuracy: 0.7598\n",
      "Epoch 66/100\n",
      "129/129 - 0s - loss: 0.5573 - accuracy: 0.7610\n",
      "Epoch 67/100\n",
      "129/129 - 0s - loss: 0.5557 - accuracy: 0.7522\n",
      "Epoch 68/100\n",
      "129/129 - 0s - loss: 0.5569 - accuracy: 0.7607\n",
      "Epoch 69/100\n",
      "129/129 - 0s - loss: 0.5524 - accuracy: 0.7644\n",
      "Epoch 70/100\n",
      "129/129 - 0s - loss: 0.5496 - accuracy: 0.7646\n",
      "Epoch 71/100\n",
      "129/129 - 0s - loss: 0.5494 - accuracy: 0.7561\n",
      "Epoch 72/100\n",
      "129/129 - 0s - loss: 0.5466 - accuracy: 0.7641\n",
      "Epoch 73/100\n",
      "129/129 - 0s - loss: 0.5444 - accuracy: 0.7651\n",
      "Epoch 74/100\n",
      "129/129 - 0s - loss: 0.5423 - accuracy: 0.7670\n",
      "Epoch 75/100\n",
      "129/129 - 0s - loss: 0.5389 - accuracy: 0.7699\n",
      "Epoch 76/100\n",
      "129/129 - 0s - loss: 0.5380 - accuracy: 0.7719\n",
      "Epoch 77/100\n",
      "129/129 - 0s - loss: 0.5343 - accuracy: 0.7721\n",
      "Epoch 78/100\n",
      "129/129 - 0s - loss: 0.5329 - accuracy: 0.7692\n",
      "Epoch 79/100\n",
      "129/129 - 0s - loss: 0.5303 - accuracy: 0.7707\n",
      "Epoch 80/100\n",
      "129/129 - 0s - loss: 0.5255 - accuracy: 0.7789\n",
      "Epoch 81/100\n",
      "129/129 - 0s - loss: 0.5270 - accuracy: 0.7777\n",
      "Epoch 82/100\n",
      "129/129 - 0s - loss: 0.5248 - accuracy: 0.7779\n",
      "Epoch 83/100\n",
      "129/129 - 0s - loss: 0.5256 - accuracy: 0.7801\n",
      "Epoch 84/100\n",
      "129/129 - 0s - loss: 0.5209 - accuracy: 0.7777\n",
      "Epoch 85/100\n",
      "129/129 - 0s - loss: 0.5161 - accuracy: 0.7842\n",
      "Epoch 86/100\n",
      "129/129 - 0s - loss: 0.5130 - accuracy: 0.7922\n",
      "Epoch 87/100\n",
      "129/129 - 0s - loss: 0.5122 - accuracy: 0.7859\n",
      "Epoch 88/100\n",
      "129/129 - 0s - loss: 0.5127 - accuracy: 0.7872\n",
      "Epoch 89/100\n",
      "129/129 - 0s - loss: 0.5115 - accuracy: 0.7847\n",
      "Epoch 90/100\n",
      "129/129 - 0s - loss: 0.5106 - accuracy: 0.7811\n",
      "Epoch 91/100\n",
      "129/129 - 0s - loss: 0.5045 - accuracy: 0.7884\n",
      "Epoch 92/100\n",
      "129/129 - 0s - loss: 0.5037 - accuracy: 0.7864\n",
      "Epoch 93/100\n",
      "129/129 - 0s - loss: 0.5017 - accuracy: 0.7910\n",
      "Epoch 94/100\n",
      "129/129 - 0s - loss: 0.4999 - accuracy: 0.7893\n",
      "Epoch 95/100\n",
      "129/129 - 0s - loss: 0.4999 - accuracy: 0.7859\n",
      "Epoch 96/100\n",
      "129/129 - 0s - loss: 0.4953 - accuracy: 0.7925\n",
      "Epoch 97/100\n",
      "129/129 - 0s - loss: 0.4934 - accuracy: 0.7947\n",
      "Epoch 98/100\n",
      "129/129 - 0s - loss: 0.4965 - accuracy: 0.7908\n",
      "Epoch 99/100\n",
      "129/129 - 0s - loss: 0.4890 - accuracy: 0.7956\n",
      "Epoch 100/100\n",
      "129/129 - 0s - loss: 0.4890 - accuracy: 0.7995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbdda55e210>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    encoded_y_train,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "particular-lighting",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 1375\n  y sizes: 4125\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-d549312ad8e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/mlenv/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1375\n  y sizes: 4125\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, encoded_y_test, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "offensive-motel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4410</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5234</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5066</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5370</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted\n",
       "4410       4          4\n",
       "2256       3          3\n",
       "3684       4          4\n",
       "4706       4          4\n",
       "5429       4          4\n",
       "1825       4          3\n",
       "3408       3          3\n",
       "4306       2          2\n",
       "386        3          4\n",
       "1751       4          4\n",
       "1381       2          3\n",
       "136        2          3\n",
       "592        5          4\n",
       "1198       3          4\n",
       "2285       3          3\n",
       "1045       3          3\n",
       "5234       4          2\n",
       "1793       4          3\n",
       "1122       3          3\n",
       "2636       4          3\n",
       "1810       4          4\n",
       "2702       3          2\n",
       "2406       3          4\n",
       "1238       4          4\n",
       "1766       2          3\n",
       "4037       2          3\n",
       "2983       3          3\n",
       "5066       3          3\n",
       "3939       4          3\n",
       "1236       3          3\n",
       "4291       3          3\n",
       "3078       3          4\n",
       "4933       4          4\n",
       "4597       2          3\n",
       "4902       3          3\n",
       "2313       4          3\n",
       "4252       4          4\n",
       "2338       3          3\n",
       "3997       4          4\n",
       "1785       3          4\n",
       "3474       3          3\n",
       "473        2          4\n",
       "5390       4          4\n",
       "589        3          4\n",
       "1028       3          4\n",
       "5370       4          4\n",
       "4107       1          1\n",
       "4205       5          4\n",
       "3660       3          3\n",
       "5470       3          2"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predicted = model.predict(X_test_scaled)\n",
    "predicted = enc.inverse_transform(predicted).flatten().tolist()\n",
    "results = pandas.DataFrame({\n",
    "    \"Actual\": y_test,\n",
    "    \"Predicted\": predicted\n",
    "})\n",
    "results.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-debut",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
